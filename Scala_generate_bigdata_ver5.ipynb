{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8a86f89-221f-43d4-b49f-e0a714f6f70b",
   "metadata": {},
   "source": [
    "# Import Neccessay Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b3a73-ab04-4f02-b7d9-181c8f671310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{Row, SaveMode, SparkSession}\n",
    "import org.apache.spark.sql.types.{IntegerType, LongType, StringType, StructField, StructType}\n",
    "import org.apache.hudi.QuickstartUtils._\n",
    "import scala.collection.JavaConversions._\n",
    "import org.apache.spark.sql.SaveMode._\n",
    "import org.apache.hudi.DataSourceReadOptions._\n",
    "import org.apache.hudi.DataSourceWriteOptions._\n",
    "import org.apache.hudi.config.HoodieWriteConfig._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.hudi.aws.sync.AwsGlueCatalogSyncTool\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "    .master(\"local[1]\")\n",
    "    .appName(\"SparkByExample\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"aws.region\", \"ap-southeast-1\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")    \n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a783066-c77e-4bec-aee1-59cbfb0fd21d",
   "metadata": {},
   "source": [
    "# Generate Big Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6512497-a567-4c31-a9c5-7bbc97dd59e3",
   "metadata": {},
   "source": [
    "## Generate Insert Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d9c77-fcec-4c08-a504-1cf0460883a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "var start_cursor: Long = 0\n",
    "for (_ <- 1 to 100) {\n",
    "    println(start_cursor)\n",
    "    var basePath = s\"s3a://sellsuki-data-lake-dev/temp/test_hudi/append_bigdata2/start_$start_cursor\"\n",
    "    println(basePath)\n",
    "    var append_df = spark.range(start_cursor, start_cursor+100000)\n",
    "      .withColumn(\"col1\", rand())\n",
    "      .withColumn(\"col2\", rand())\n",
    "      .withColumn(\"col3\", rand())\n",
    "      .withColumn(\"col4\", rand())\n",
    "      .withColumn(\"col5\", rand())\n",
    "\n",
    "    var append_dfWithPartition = append_df\n",
    "        .withColumn(\"Partition\", (col(\"col1\") * 1000000 / 10000).cast(\"Integer\"))\n",
    "        .withColumn(\"Partition2\", (col(\"col1\") * 1000000 / 100000).cast(\"Integer\"))\n",
    "\n",
    "    var temp = append_dfWithPartition.select(max(\"id\")).collect()\n",
    "\n",
    "    append_dfWithPartition.write.parquet(basePath)\n",
    "\n",
    "    if (temp(0)(0) != null) {\n",
    "        start_cursor = temp(0).getLong(0) + 1\n",
    "    } else {\n",
    "        start_cursor = 0 // or any other value you want to set when the DataFrame is empty\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18b72106-f7a7-4a6b-8df8-fd078deee3e1",
   "metadata": {},
   "source": [
    "## Generate Update Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d1bef-1379-49b3-a751-815eac58f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "var start_cursor: Long = 0\n",
    "for (_ <- 1 to 10) {\n",
    "    println(start_cursor)\n",
    "    var basePath = s\"s3a://sellsuki-data-lake-dev/temp/test_hudi/append_bigdata3/start_$start_cursor\"\n",
    "    println(basePath)\n",
    "    var append_df = spark.range(start_cursor, start_cursor+100000)\n",
    "      .withColumn(\"col1\", rand())\n",
    "      .withColumn(\"col2\", rand())\n",
    "      .withColumn(\"col3\", rand())\n",
    "      .withColumn(\"col4\", rand())\n",
    "      .withColumn(\"col5\", rand())\n",
    "\n",
    "    var append_dfWithPartition = append_df\n",
    "        .withColumn(\"Partition\", (col(\"col1\") * 1000000 / 10000).cast(\"Integer\"))\n",
    "        .withColumn(\"Partition2\", (col(\"col1\") * 1000000 / 100000).cast(\"Integer\"))\n",
    "\n",
    "    var temp = append_dfWithPartition.select(max(\"id\")).collect()\n",
    "\n",
    "    append_dfWithPartition.write.parquet(basePath)\n",
    "\n",
    "    if (temp(0)(0) != null) {\n",
    "        start_cursor = temp(0).getLong(0) + 1\n",
    "    } else {\n",
    "        start_cursor = 0 // or any other value you want to set when the DataFrame is empty\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1217875-b7db-4d51-ad55-f446cc99fa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "var start_cursor: Long = 0\n",
    "for (_ <- 1 to 100) {\n",
    "    println(start_cursor)\n",
    "    var basePath = s\"s3a://sellsuki-data-lake-dev/temp/test_hudi/append_bigdata2_1/start_$start_cursor\"\n",
    "    println(basePath)\n",
    "    var append_df = spark.range(start_cursor, start_cursor+10000)\n",
    "      .withColumn(\"col1\", rand())\n",
    "      .withColumn(\"col2\", rand())\n",
    "      .withColumn(\"col3\", rand())\n",
    "      .withColumn(\"col4\", rand())\n",
    "      .withColumn(\"col5\", rand())\n",
    "\n",
    "    var append_dfWithPartition = append_df\n",
    "        .withColumn(\"Partition\", (col(\"col1\") * 1000000 / 10000).cast(\"Integer\"))\n",
    "        .withColumn(\"Partition2\", (col(\"col1\") * 1000000 / 100000).cast(\"Integer\"))\n",
    "\n",
    "    var temp = append_dfWithPartition.select(max(\"id\")).collect()\n",
    "\n",
    "    append_dfWithPartition.write.parquet(basePath)\n",
    "\n",
    "    if (temp(0)(0) != null) {\n",
    "        start_cursor = temp(0).getLong(0) + 1\n",
    "    } else {\n",
    "        start_cursor = 0 // or any other value you want to set when the DataFrame is empty\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0fb2b45-4c2c-4ee7-82ad-8c7ff3eb55f1",
   "metadata": {},
   "source": [
    "# Write MOR Hudi Table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbf85a45-a7bb-4740-b6cd-dcd92c4b5b9b",
   "metadata": {},
   "source": [
    "## 10m Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73922704-ad82-41da-9198-3c565e47b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{Row, SaveMode, SparkSession}\n",
    "import org.apache.spark.sql.types.{IntegerType, LongType, StringType, StructField, StructType}\n",
    "import org.apache.hudi.QuickstartUtils._\n",
    "import scala.collection.JavaConversions._\n",
    "import org.apache.spark.sql.SaveMode._\n",
    "import org.apache.hudi.DataSourceReadOptions._\n",
    "import org.apache.hudi.DataSourceWriteOptions._\n",
    "import org.apache.hudi.config.HoodieWriteConfig._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.hudi.aws.sync.AwsGlueCatalogSyncTool\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "    .master(\"local[1]\")\n",
    "    .appName(\"SparkByExample\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"aws.region\", \"ap-southeast-1\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")    \n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate();\n",
    "\n",
    "var start_cursor: Long = 0\n",
    "var parqDF = spark.read.parquet(\"s3a://data-lake-dev/temp/test_hudi/append_bigdata2/start_0\")\n",
    "val startTime_3 = System.currentTimeMillis()\n",
    "for (_ <- 1 to 100) {\n",
    "    println(start_cursor)\n",
    "    var index = start_cursor*100000\n",
    "    val tableName = \"big_data_mor_hudi\"\n",
    "    val tableBase = \"s3a://data-lake-dev/temp/test_hudi/v5_big_data_mor_hudi\"\n",
    "    var basePath = s\"s3a://data-lake-dev/temp/test_hudi/append_bigdata2/start_$index\"\n",
    "    println(basePath)\n",
    "\n",
    "    val parqDF = spark.read.parquet(basePath)\n",
    "\n",
    "    parqDF.write.format(\"hudi\").\n",
    "      options(getQuickstartWriteConfigs).\n",
    "      option(\"hoodie.datasource.write.keygenerator.class\", \"org.apache.hudi.keygen.SimpleKeyGenerator\").\n",
    "      option(\"hoodie.compact.inline.max.delta.seconds\", \"86400\").\n",
    "      option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL).\n",
    "      option(TABLE_TYPE.key, MOR_TABLE_TYPE_OPT_VAL).\n",
    "      option(PRECOMBINE_FIELD_OPT_KEY, \"Partition2\").\n",
    "      option(PARTITIONPATH_FIELD_OPT_KEY, \"Partition2\").\n",
    "      option(RECORDKEY_FIELD_OPT_KEY, \"id\").\n",
    "      option(\"hoodie.table.name\", tableName).\n",
    "      option(\"hoodie.datasource.write.hive_style_partitioning\",\"true\").\n",
    "      mode(Append).\n",
    "      save(tableBase)\n",
    "\n",
    "    start_cursor = start_cursor + 1\n",
    "}\n",
    "\n",
    "val endTime_3 = System.currentTimeMillis()\n",
    "val executionTime_3 = endTime_3 - startTime_3\n",
    "println(s\"Append MOR Execution time: $executionTime_3 ms\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ba99ef0",
   "metadata": {},
   "source": [
    "- Append MOR Execution time: 2328205 ms   ~ 38.80341667 min   846.8 MB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42fb0376-683b-4e53-a4b0-0c070724d6f4",
   "metadata": {},
   "source": [
    "## 1m Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa9cc22-f114-412c-99e5-af45aaed8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{Row, SaveMode, SparkSession}\n",
    "import org.apache.spark.sql.types.{IntegerType, LongType, StringType, StructField, StructType}\n",
    "import org.apache.hudi.QuickstartUtils._\n",
    "import scala.collection.JavaConversions._\n",
    "import org.apache.spark.sql.SaveMode._\n",
    "import org.apache.hudi.DataSourceReadOptions._\n",
    "import org.apache.hudi.DataSourceWriteOptions._\n",
    "import org.apache.hudi.config.HoodieWriteConfig._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.hudi.aws.sync.AwsGlueCatalogSyncTool\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "    .master(\"local[1]\")\n",
    "    .appName(\"SparkByExample\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"aws.region\", \"ap-southeast-1\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")    \n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate();\n",
    "\n",
    "var start_cursor: Long = 0\n",
    "var parqDF = spark.read.parquet(\"s3a://sellsuki-data-lake-dev/temp/test_hudi/append_bigdata2_1/start_0\")\n",
    "val startTime_3 = System.currentTimeMillis()\n",
    "for (_ <- 1 to 100) {\n",
    "    println(start_cursor)\n",
    "    var index = start_cursor*10000\n",
    "    val tableName = \"big_data_mor_hudi\"\n",
    "    val tableBase = \"s3a://sellsuki-data-lake-dev/temp/test_hudi/v5_big_data_mor_hudi2_1\"\n",
    "    var basePath = s\"s3a://sellsuki-data-lake-dev/temp/test_hudi/append_bigdata2_1/start_$index\"\n",
    "    println(basePath)\n",
    "\n",
    "    val parqDF = spark.read.parquet(basePath)\n",
    "\n",
    "    parqDF.write.format(\"hudi\").\n",
    "      options(getQuickstartWriteConfigs).\n",
    "      option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL).\n",
    "      option(\"hoodie.datasource.write.keygenerator.class\", \"org.apache.hudi.keygen.SimpleKeyGenerator\").\n",
    "      option(\"hoodie.compact.inline.max.delta.seconds\", \"86400\").\n",
    "      option(TABLE_TYPE.key, MOR_TABLE_TYPE_OPT_VAL).\n",
    "      option(PRECOMBINE_FIELD_OPT_KEY, \"Partition2\").\n",
    "      option(PARTITIONPATH_FIELD_OPT_KEY, \"Partition2\").\n",
    "      option(RECORDKEY_FIELD_OPT_KEY, \"id\").\n",
    "      option(\"hoodie.table.name\", tableName).\n",
    "      option(\"hoodie.datasource.write.hive_style_partitioning\",\"true\").\n",
    "      mode(Append).\n",
    "      save(tableBase)\n",
    "\n",
    "    start_cursor = start_cursor + 1\n",
    "}\n",
    "\n",
    "val endTime_3 = System.currentTimeMillis()\n",
    "val executionTime_3 = endTime_3 - startTime_3\n",
    "println(s\"Append MOR Execution time: $executionTime_3 ms\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3144bf5-160f-4da9-afca-d74c6153922a",
   "metadata": {},
   "source": [
    "# Write COW Hudi Table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f03bdefe-e0dc-49fd-920c-b607ef1f7a05",
   "metadata": {},
   "source": [
    "## 10M Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4465abd-fad0-440b-b1d4-b5cb2908030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{Row, SaveMode, SparkSession}\n",
    "import org.apache.spark.sql.types.{IntegerType, LongType, StringType, StructField, StructType}\n",
    "import org.apache.hudi.QuickstartUtils._\n",
    "import scala.collection.JavaConversions._\n",
    "import org.apache.spark.sql.SaveMode._\n",
    "import org.apache.hudi.DataSourceReadOptions._\n",
    "import org.apache.hudi.DataSourceWriteOptions._\n",
    "import org.apache.hudi.config.HoodieWriteConfig._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.hudi.aws.sync.AwsGlueCatalogSyncTool\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "    .master(\"local[1]\")\n",
    "    .appName(\"SparkByExample\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"aws.region\", \"ap-southeast-1\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")    \n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate();\n",
    "\n",
    "var start_cursor: Long = 0\n",
    "var parqDF = spark.read.parquet(\"s3a://data-lake-dev/temp/test_hudi/append_bigdata2/start_0\")\n",
    "\n",
    "val startTime_3 = System.currentTimeMillis()\n",
    "for (_ <- 1 to 100) {\n",
    "    println(start_cursor)\n",
    "    var index = start_cursor*100000\n",
    "    val tableName = \"big_data_cow_hudi\"\n",
    "    val tableBase = \"s3a://data-lake-dev/temp/test_hudi/v5_big_data_cow_hudi2\"\n",
    "    var basePath = s\"s3a://data-lake-dev/temp/test_hudi/append_bigdata2/start_$index\"\n",
    "    println(basePath)\n",
    "\n",
    "    val parqDF = spark.read.parquet(basePath)\n",
    "\n",
    "    parqDF.write.format(\"hudi\").\n",
    "      options(getQuickstartWriteConfigs).\n",
    "      option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL).\n",
    "      option(\"hoodie.datasource.write.keygenerator.class\", \"org.apache.hudi.keygen.SimpleKeyGenerator\").\n",
    "      option(TABLE_TYPE.key, COW_TABLE_TYPE_OPT_VAL).\n",
    "      option(PRECOMBINE_FIELD_OPT_KEY, \"Partition2\").\n",
    "      option(PARTITIONPATH_FIELD_OPT_KEY, \"Partition2\").\n",
    "      option(RECORDKEY_FIELD_OPT_KEY, \"id\").\n",
    "      option(\"hoodie.table.name\", tableName).\n",
    "      option(\"hoodie.datasource.write.hive_style_partitioning\",\"true\").\n",
    "      mode(Append).\n",
    "      save(tableBase)\n",
    "\n",
    "    start_cursor = start_cursor + 1\n",
    "\n",
    "}\n",
    "val endTime_3 = System.currentTimeMillis()\n",
    "val executionTime_3 = endTime_3 - startTime_3\n",
    "println(s\"Append COW Execution time: $executionTime_3 ms\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfa0782f",
   "metadata": {},
   "source": [
    "- Append COW Execution time: 2147978 ms   ~ 35.79963333 min   846.8 MB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f344fe2d-d7f2-4555-ad12-51a389cdb12c",
   "metadata": {},
   "source": [
    "## 1M Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e285993e-d268-4808-8b25-3f20b02152e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{Row, SaveMode, SparkSession}\n",
    "import org.apache.spark.sql.types.{IntegerType, LongType, StringType, StructField, StructType}\n",
    "import org.apache.hudi.QuickstartUtils._\n",
    "import scala.collection.JavaConversions._\n",
    "import org.apache.spark.sql.SaveMode._\n",
    "import org.apache.hudi.DataSourceReadOptions._\n",
    "import org.apache.hudi.DataSourceWriteOptions._\n",
    "import org.apache.hudi.config.HoodieWriteConfig._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.hudi.aws.sync.AwsGlueCatalogSyncTool\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "    .master(\"local[1]\")\n",
    "    .appName(\"SparkByExample\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"aws.region\", \"ap-southeast-1\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")    \n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate();\n",
    "\n",
    "var start_cursor: Long = 0\n",
    "var parqDF = spark.read.parquet(\"s3a://sellsuki-data-lake-dev/temp/test_hudi/append_bigdata2/start_0\")\n",
    "\n",
    "val startTime_3 = System.currentTimeMillis()\n",
    "for (_ <- 1 to 100) {\n",
    "    println(start_cursor)\n",
    "    var index = start_cursor*10000\n",
    "    val tableName = \"big_data_cow_hudi\"\n",
    "    val tableBase = \"s3a://sellsuki-data-lake-dev/temp/test_hudi/v5_big_data_cow_hudi2_1\"\n",
    "    var basePath = s\"s3a://sellsuki-data-lake-dev/temp/test_hudi/append_bigdata2_1/start_$index\"\n",
    "    println(basePath)\n",
    "\n",
    "    val parqDF = spark.read.parquet(basePath)\n",
    "\n",
    "    parqDF.write.format(\"hudi\").\n",
    "      options(getQuickstartWriteConfigs).\n",
    "      option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL).\n",
    "      option(\"hoodie.datasource.write.keygenerator.class\", \"org.apache.hudi.keygen.SimpleKeyGenerator\").\n",
    "      option(TABLE_TYPE.key, COW_TABLE_TYPE_OPT_VAL).\n",
    "      option(PRECOMBINE_FIELD_OPT_KEY, \"Partition2\").\n",
    "      option(PARTITIONPATH_FIELD_OPT_KEY, \"Partition2\").\n",
    "      option(RECORDKEY_FIELD_OPT_KEY, \"id\").\n",
    "      option(\"hoodie.table.name\", tableName).\n",
    "      option(\"hoodie.datasource.write.hive_style_partitioning\",\"true\").\n",
    "      mode(Append).\n",
    "      save(tableBase)\n",
    "\n",
    "    start_cursor = start_cursor + 1\n",
    "\n",
    "}\n",
    "val endTime_3 = System.currentTimeMillis()\n",
    "val executionTime_3 = endTime_3 - startTime_3\n",
    "println(s\"Append COW Execution time: $executionTime_3 ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
